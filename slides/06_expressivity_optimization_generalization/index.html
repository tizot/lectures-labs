<!DOCTYPE html>
<html>
  <head>
    <title>Deep Learning Lectures</title>
    <meta charset="utf-8">
    <style>
     .left-column {
       width: 50%;
       float: left;
     }
     .reset-column {
       overflow: auto;
        width: 100%;
     }
     .small { font-size: 0.2em; }
     .right-column {
       width: 50%;
       float: right;
     }
     .footnote {
        position: absolute;
        bottom: 2em;
        margin: 0em 2em;
      }
     .grey { color: #bbbbbb; }
      </style>
    <link rel="stylesheet" type="text/css" href="slides.css">
  </head>
  <body>
    <textarea id="source">
class: center, middle

# Learning with Deep Networks: Expressivity, Optimization &amp; Generalization

Charles Ollion - Olivier Grisel

.affiliations[
  ![Heuritech](images/logo heuritech v2.png)
  ![Inria](images/inria-logo.png)
  ![UPS](images/Logo_Master_Datascience.png)
]

---
# Decomposition of the Error

.left-column[
$$\begin{eqnarray}
E\_n(f\_n) - E(f^\star) & = & E(f\_{\mathcal{F}}^\star) - E(f^\star) \\\\
& + & E\_n(f\_n) - E(f\_{\mathcal{F}}^\star)
\end{eqnarray}$$
]

.right-column[.small[
- Approximation error
- Estimation error
]]

.reset-column[
]

--

Ground truth function: $f^\star$

--

Hypothesis class: $\mathcal{F}$, best hypothesis: $f\_\mathcal{F}^\star \in \mathcal{F}$

--

Expected Risk: $E(f) = \int l(f(x), y).dP(x, y)$ (but $P$ is unknown)

--

Empirical Risk: $E_n(f) = \frac{1}{n} \sum_i l(f(x\_i), y\_i)$

--

Empirical Risk Minimization:

$$ f\_n = argmin\_{f \in \mathcal{F}} E\_n(f) $$


???
Question: how is the approximation error impacted by:
          - a larger hypothesis class $\mathcal{F}$?

Question: how is the estimation error impacted by:
          - a larger training set $n$?
          - a larger hypothesis class $\mathcal{F}$?

---
# Decomposition of the Error

.left-column[
$$\begin{eqnarray}
E\_n(\tilde{f}\_n) - E(f^\star) & = & E(f\_{\mathcal{F}}^\star) - E(f^\star) \\\\
& + & E\_n(f\_n) - E(f\_{\mathcal{F}}^\star) \\\\
& + & E\_n(\tilde{f}\_n) - E\_n(f\_n)
\end{eqnarray}$$
]

.right-column[.small[
- Approximation error
- Estimation error
- Optimization error
]]

.reset-column[
]

.footnote.small[
Learning using Large Datasets, L. Bottou &amp; O. Bousquet, 2008.
]

--

Computing $argmin\_{f \in \mathcal{F}} E\_n(f)$ exactly can be very costly...

--

or even untractable e.g. for non-convex objective: $\theta \rightarrow
l(f\_{\theta}(x\_i), y\_i)$

--

In practice: approximate optimizer with tolerance $\rho$ that finds
 $\tilde{f}\_n$:

$$ E\_n(\tilde{f}\_n) < E\_n(f\_n) + \rho $$

Examples: `max_iter` and `tol` in sklearn, `nb_epoch` in keras, SGD with
constant step size and early stopping...
 

???
**Question**: Where do Underfitting and Overfitting problems fit in this
              framework?

**Question**: Which error(s) do you expect to dominate when fitting
              linear (logistic regression) model for image
              classification?

---
# Decomposition of the Error

**Approximation error**:
- decreases when $\mathcal{F}$ increases;
- but typically bounded by computational constraints.

--

**Estimation error**:
- decreases when $n$ increases;
- can increase when $\mathcal{F}$ increases (according to VC theory).

--

**Approximation error**:
- can increases when tolerance $\rho$ increases;
- can increase when $\mathcal{F}$ gets more complex (non-convex obj.).

???
Examples of computational constraints: training time, size of the model
parameters and activations in GPU RAM or mobile phone RAM, prediction
latency...

**Question** how do rho, n and F impact the compute time?

---
# Outline

<br/>


### Expressivity and Universal Function Approximation

--

### Optimization

--

### Generalization

---
class: middle, center

# Expressivity and Universal Function Approximation

---
## Universal Function Approximation

Let $\sigma$ be a nonconstant, bounded, and monotonically-increasing
continuous function. For any $f \in C([0, 1]^{d})$ and $\varepsilon >
0$, there exists $h \in \mathbb{N}$ real constants $v\_i, b\_i \in
\mathbb{R}$ and real vectors $w_i \in \mathbb{R}^d$ such that:

$$ | \sum\_i^h v\_i \sigma(w\_i^Tx + b\_i) - f (x) | < \varepsilon  $$

that is: neural nets are dense in $C([0, 1]^{d})$.

This still holds for any compact subset of $\mathbb{R}^d$ and if
$\sigma$ is the ReLU function (Sonoda &amp; Murata, 2015).

.footnote.small[
Approximation Capabilities of Multilayer Feedforward Networks,
K. Hornik, 1991
]

---
# Problem solved?

UFA theorems **do not tell us**:

- The number $h$ of hidden units is small enough to have the network fit
  in RAM.

- The optimal function parameters can be found in finite time by
  minimizing the Empirical Risk with SGD and the usual random
  initialization schemes.

???
A finite smple version of the theorem is constructive enough to fit a
MLP with $2n + d$ parameters where $n$ is the number of points in the
dataset. That requires running a $n$ by $n$ system though.

https://arxiv.org/abs/1611.03530

---
# Approximation with ReLU

.footnote.small[
[Quora: Is a single layered ReLU network still a universal
approximator?](https://www.quora.com/Is-a-single-layered-ReLu-network-still-a-universal-approximator),
Conner Davis]

---
# Depth can help reduce $h$

[Matus Telgarsky, 2016](https://arxiv.org/abs/1602.04485): There exists
functions that can be approximated by a deep ReLU network with
$\Theta(k^3)$ layers with a $\Theta(1)$ units that cannot be
approximated by shallower networks with $\Theta(k)$ layers unless they
have $\Omega(2^k)$ units.


???

This also holds for ReLU convnets with max pooling layers and other
polynomial-based architectures.

---
# For a fixed param budget, deeper is better

.center[
<img alt="depth 2 vs depth 1 net" src="images/depth-2-vs-depth-1.png"
 width="90%" />
]

.footnote.small[
[On the Number of Linear Regions of Deep Neural Networks](
  https://arxiv.org/abs/1402.1869), G. MontÃºfar, R. Pascanu, K. Cho,
  Y. Bengio, 2014.
]

???

Note: deeper is better is true from a function approximation point of
view and probably from a generalization point of view but not
necessarily from an optimization point of view.

---
# Expressivity (DRAFT)

- Universal Continuous Function Approximation

- Finite domain approximation

- Depth vs Width and parametric cost

---
class: middle, center

# Optimization for Deep Networks

---
# Optimization (DRAFT)

- Why SGD?

- Local Minima

  - Intuition: overparametrization => easy overfitting => all minima
    have zero training cost and are therefore global

- Optimization issues related to depth

  - Initialization / Residual Connections

- Conditioning / Natural Gradient

  - Adam / Batch Normalization

- Remaining optimization problems:

  - Still slow on large vision problems: e.g.

  - RL with Policy Gradient: too much noise: no training


---
# Batch Normalization

Normalize activations in each **mini-batch** before activation function: **speeds up** and **stabilizes** training (less dependant on initialization)

.footnote.small[
Ioffe, Sergey, and Christian Szegedy. "Batch normalization: Accelerating deep network training by reducing internal covariate shift." ICML 2015
]

--
.center[
<img src="images/batchnorm.png" style="width: 450px;" />
]

---
# Batch Normalization

At **inference time**, use average and standard deviation computed on **the whole dataset** instead of batch

--

Widely used in **ConvNets**, but requires the mini-batch to be large enough to compute statistics in the minibatch

--

Much less effective in RNNs (but variants exist)

--

In **Keras**: use it as a layer, before activation	
	
```python
x = Convolution2D(64, 1, 1)(input_tensor)
x = BatchNormalization()(x)
x = Activation('relu')(x)
```

--

- Introduces new parameters: `2 x size_of_activation` (here 128)

--
- Keras keeps track of a **running average/std** of activations for inference, no need to have specific inference code.

---
# Layer Normalizations

Normalize on the statistics of the **layer activations** instead of mini-batch

.center[
<img src="images/layernorm.png" style="width: 400px;" />
]

.footnote.small[
Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. "Layer normalization." 2016.<br/>
]

--

The algorithm is then similar as Batch Normalization

--

Suited for **RNNs**, degrades performance of **CNNs**

---
# Weight Normalization

Reparametrize weights of neurons, to decouple **direction** and **norm** of the weight:

$$
\mathbf{w}  = \frac{g}{||\mathbf{v}||}\mathbf{v}
$$

.footnote.small[
Salimans, Tim, and Diederik P. Kingma. "Weight normalization: A simple reparameterization to accelerate training of deep neural networks." NIPS 2016.	
]

--


One new parameter $g$ to learn per neuron

--

Careful **data-based** initialization of $g$ and neuron bias $b$ is better (not applicable to RNNs)

---
# Reparametrizations and Normalizations

**Significant impact** on results

.footnote.small[
Ren, Mengye, et al. "Normalizing the Normalizers: Comparing and Extending Network Normalization Schemes." 2017
]

- Convergence speed and better optimum on CNNs (BN mostly used)

--
- Layer Norm strongly impacts LSTM in language modelling tasks

--
- Most generative models are almost impossible to train without Batch Normalization

--

Active area of research - See "Normalizing the Normalizers"

---
class: middle, center

# Generalization

---
# Generalization (DRAFT)

- Memorization vs Generalization

- Importance of small batch sizes

- Dropout and Gradient noise

- Early stopping as implicit regularization

  - Computational constraints as implicit regularization

---
# Conclusions

- A strong optimizer is not necessarily a strong learner

- Neural Networks are non-convex but local minima are rarely a problem

- Neural Networks are over-parametrized but can still generalize

- Stochastic Gradient solvers is a strong implicit regularizer

- Open area of research

---
class: middle, center

# Lab #6: Room C48 and C49 in 15min!

    </textarea>
    <style TYPE="text/css">
      code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
    </style>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
      tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
      }
      });
      MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i = 0; i < all.length; i += 1) {
		     all[i].SourceElement().parentNode.className += ' has-jax';
		     }
		     });
		     </script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="../remark.min.js" type="text/javascript">
    </script>
    <script type="text/javascript">
      var slideshow = remark.create({
        highlightStyle: 'github',
        highlightSpans: true,
        highlightLines: true
      });
    </script>
  </body>
</html>
