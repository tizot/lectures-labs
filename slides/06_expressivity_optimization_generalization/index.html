<!DOCTYPE html>
<html>
  <head>
    <title>Deep Learning Lectures</title>
    <meta charset="utf-8">
    <style>
     .left-column {
       width: 50%;
       float: left;
     }
     .reset-column {
       overflow: auto;
        width: 100%;
     }
     .small { font-size: 0.2em; }
     .right-column {
       width: 50%;
       float: right;
     }
     .footnote {
        position: absolute;
        bottom: 2em;
        margin: 0em 2em;
      }
     .grey { color: #bbbbbb; }
      </style>
    <link rel="stylesheet" type="text/css" href="slides.css">
  </head>
  <body>
    <textarea id="source">
class: center, middle

# Learning with Deep Networks: Expressivity, Optimization &amp; Generalization

Charles Ollion - Olivier Grisel

.affiliations[
  ![Heuritech](images/logo heuritech v2.png)
  ![Inria](images/inria-logo.png)
  ![UPS](images/Logo_Master_Datascience.png)
]

---
# Decomposition of the Error

.left-column[
$$\begin{eqnarray}
E\_n(f\_n) - E(f^\star) & = & E(f\_{\mathcal{F}}^\star) - E(f^\star) \\\\
& + & E\_n(f\_n) - E(f\_{\mathcal{F}}^\star)
\end{eqnarray}$$
]

.right-column[.small[
- Approximation error
- Estimation error
]]

.reset-column[
]

--

Ground truth function: $f^\star$

--

Hypothesis class: $\mathcal{F}$, best hypothesis: $f\_\mathcal{F}^\star \in \mathcal{F}$

--

Expected Risk: $E(f) = \int l(f(x), y).dP(x, y)$ (but $P$ is unknown)

--

Empirical Risk: $E_n(f) = \frac{1}{n} \sum_i l(f(x\_i), y\_i)$

--

Empirical Risk Minimization:

$$ f\_n = argmin\_{f \in \mathcal{F}} E\_n(f) $$


???
Question: how is the approximation error impacted by:
          - a larger hypothesis class $\mathcal{F}$?

Question: how is the estimation error impacted by:
          - a larger training set $n$?
          - a larger hypothesis class $\mathcal{F}$?

---
# Decomposition of the Error

.left-column[
$$\begin{eqnarray}
E\_n(\tilde{f}\_n) - E(f^\star) & = & E(f\_{\mathcal{F}}^\star) - E(f^\star) \\\\
& + & E\_n(f\_n) - E(f\_{\mathcal{F}}^\star) \\\\
& + & E\_n(\tilde{f}\_n) - E\_n(f\_n)
\end{eqnarray}$$
]

.right-column[.small[
- Approximation error
- Estimation error
- Optimization error
]]

.reset-column[
]

.footnote.small[
Learning using Large Datasets, L. Bottou &amp; O. Bousquet, 2008.
]

--

Computing $f\_n = argmin\_{f \in \mathcal{F}} E\_n(f)$ can be too costly...

--

or even untractable e.g. for non-convex objective: $\theta \rightarrow
l(f\_{\theta}(x\_i), y\_i)$

--

In practice: approximate optimizer with tolerance $\rho$ that finds
$\tilde{f}\_n$:

$$ E\_n(\tilde{f}\_n) < E\_n(f\_n) + \rho $$

Examples: `max_iter` and `tol` in sklearn, `nb_epoch` in keras, SGD with
constant step size and early stopping...


???
**Question**: Where do Underfitting and Overfitting problem fit in this
             framework?

**Question**: Which error(s) do you expect to dominate when fitting
              linear (logistic regression) model for image
              classification?

---
# Decomposition of the Error

**Approximation error**:
- decreases when $\mathcal{F}$ increases;
- but typically bounded by computational constraints.

--

**Estimation error**:
- decreases when $n$ increases;
- can increase when $\mathcal{F}$ increases (according to VC theory).

--

**Approximation error**:
- can increases when tolerance $\rho$ increases;
- can increase when $\mathcal{F}$ gets more complex (non-convex obj.).

???
Examples of computational constraints: training time, size of the model
parameters and activations in GPU RAM or mobile phone RAM, prediction
latency...

**Question** how do rho, n and F impact the compute time?

---
# Outline

<br/>


### Expressivity and Universal Function Approximation

--

### Optimization

--

### Generalization

---
class: middle, center

# Expressivity and Universal Function Approximation

---
# Expressivity (DRAFT)

- Universal Continuous Function Approximation

- Finite domain approximation

- Depth vs Width and parametric cost

---
class: middle, center

# Optimization for Deep Networks

---
# Optimization (DRAFT)

- Why SGD?

- Local Minima

  - Intuition: overparametrization => easy overfitting => all minima
    have zero training cost and are therefore global

- Optimization issues related to depth

  - Initialization / Residual Connections

- Conditioning / Natural Gradient

  - Adam / Batch Normalization

- Remaining optimization problems:

  - Still slow on large vision problems: e.g.

  - RL with Policy Gradient: too much noise: no training

---
class: middle, center

# Generalization

---
# Generalization (DRAFT)

- Memorization vs Generalization

- Importance of small batch sizes

- Dropout and Gradient noise

- Early stopping as implicit regularization

  - Computational constraints as implicit regularization

---
# Conclusions

- A strong optimizer is not necessarily a strong learner

- Neural Networks are non-convex but local minima are rarely a problem

- Neural Networks are over-parametrized but can still generalize

- Stochastic Gradient solvers is a strong implicit regularizer

- Open area of research

---
class: middle, center

# Lab #6: Room C48 and C49 in 15min!

    </textarea>
    <style TYPE="text/css">
      code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
    </style>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
      tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
      }
      });
      MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i = 0; i < all.length; i += 1) {
		     all[i].SourceElement().parentNode.className += ' has-jax';
		     }
		     });
		     </script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="../remark.min.js" type="text/javascript">
    </script>
    <script type="text/javascript">
      var slideshow = remark.create({
        highlightStyle: 'github',
        highlightSpans: true,
        highlightLines: true
      });
    </script>
  </body>
</html>
