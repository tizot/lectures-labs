{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation of Numeric Phrases with Seq2Seq\n",
    "\n",
    "In the following we will try to build a translation model from french phrases describing numbers to the corresponding digital representation (base 10).\n",
    "\n",
    "The parallel text data is generated from a \"ground-truth\" Python function named `to_french_phrase` that captures common rules from the French language except hypenation to make the French strings more ambiguous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from french_numbers import to_french_phrase\n",
    "\n",
    "\n",
    "for x in [21, 80, 81, 300, 213, 1100, 1201, 301000, 80080]:\n",
    "    print(str(x).rjust(6), to_french_phrase(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Training Set\n",
    "\n",
    "The following will generate phrases 20000 example phrases for numbers between 1 and 1,000,000 (excluded). It will over-represent small numbers by generating all the possible short sequences between 1 and `exhaustive`.\n",
    "\n",
    "Let's split the generated set into non-overlapping train, validation and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from french_numbers import generate_translations\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "numbers, french_numbers = generate_translations(\n",
    "    low=1, high=int(1e6) - 1, exhaustive=5000, random_seed=0)\n",
    "num_train, num_dev, fr_train, fr_dev = train_test_split(\n",
    "    numbers, french_numbers, test_size=0.5, random_state=0)\n",
    "\n",
    "num_val, num_test, fr_val, fr_test = train_test_split(\n",
    "    num_dev, fr_dev, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(fr_train), len(fr_val), len(fr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, fr_phrase, num_phrase in zip(range(5), fr_train, num_train):\n",
    "    print(num_phrase.rjust(6), fr_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, fr_phrase, num_phrase in zip(range(5), fr_val, num_val):\n",
    "    print(num_phrase.rjust(6), fr_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabularies\n",
    "\n",
    "Build the vocabularies from the training set only to get a chance to have some out-of-vocabulary words in the validation and test sets.\n",
    "\n",
    "First we need to introduce specific symbols that will be used to:\n",
    "- pad sequences\n",
    "- mark the beginning of translation\n",
    "- mark the end of translation\n",
    "- be used as a placehold for out-of-vocabulary symbols (not seen in the training set).\n",
    "\n",
    "Here we use the same convention as the [tensorflow seq2seq tutorial](https://www.tensorflow.org/tutorials/seq2seq):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD, GO, EOS, UNK = START_VOCAB = ['_PAD', '_GO', '_EOS', '_UNK']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the vocabulary we need to tokenize the sequences of symbols. For the digital number representation we use character level tokenization while whitespace-based word level tokenization will do for the French phrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(sentence, word_level=True):\n",
    "    if word_level:\n",
    "        return sentence.split()\n",
    "    else:\n",
    "        return [sentence[i:i + 1] for i in range(len(sentence))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenize('1234', word_level=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenize('mille deux cent trente quatre', word_level=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use this tokenization strategy to assign a unique integer token id to each possible token string found the traing set in each language ('French' and 'numeric'): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(tokenized_sequences):\n",
    "    rev_vocabulary = START_VOCAB[:]\n",
    "    unique_tokens = set()\n",
    "    for tokens in tokenized_sequences:\n",
    "        unique_tokens.update(tokens)\n",
    "    rev_vocabulary += sorted(unique_tokens)\n",
    "    vocabulary = {}\n",
    "    for i, token in enumerate(rev_vocabulary):\n",
    "        vocabulary[token] = i\n",
    "    return vocabulary, rev_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_fr_train = [tokenize(s, word_level=True) for s in fr_train]\n",
    "tokenized_num_train = [tokenize(s, word_level=False) for s in num_train]\n",
    "\n",
    "fr_vocab, rev_fr_vocab = build_vocabulary(tokenized_fr_train)\n",
    "num_vocab, rev_num_vocab = build_vocabulary(tokenized_num_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two languages do not have the same vocabulary sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(fr_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(num_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k, v in sorted(fr_vocab.items())[:10]:\n",
    "    print(k.rjust(10), v)\n",
    "print('...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k, v in sorted(num_vocab.items()):\n",
    "    print(k.rjust(10), v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also built the reverse mappings from token ids to token string representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(rev_fr_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(rev_num_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq with a single GRU architecture\n",
    "\n",
    "<img src=\"images/basic_seq2seq.png\" width=\"80%\" />\n",
    "\n",
    "From: [Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. \"Sequence to sequence learning with neural networks.\" NIPS 2014](https://arxiv.org/abs/1409.3215)\n",
    "\n",
    "\n",
    "\n",
    "For a given source sequence - target sequence pair, we will:\n",
    "- tokenize the source and target sequences;\n",
    "- reverse the order of the source sequence;\n",
    "- build the input sequence by concatenating the reversed source sequence and the target sequence in original order using the `_GO` token as a delimiter, \n",
    "- build the output sequence by appending the `_EOS` token to the source sequence.\n",
    "\n",
    "\n",
    "Let's do this as a function using the original string representations for the tokens so as to make it easier to debug:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_input_output(source_tokens, target_tokens, reverse_source=True):\n",
    "    if reverse_source:\n",
    "        source_tokens = source_tokens[::-1]\n",
    "    input_tokens = source_tokens + [GO] + target_tokens\n",
    "    output_tokens = target_tokens + [EOS]\n",
    "    return input_tokens, output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_tokens, output_tokens = make_input_output(\n",
    "    ['cent', 'vingt', 'et', 'un'],\n",
    "    ['1', '2', '1'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization of the parallel corpus\n",
    "\n",
    "Let's apply the previous transformation to each pair of (source, target) sequene and use a shared vocabulary to store the results in numpy arrays of integer token ids, with padding on the left so that all input / output sequences have the same length: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_tokenized_sequences = tokenized_fr_train + tokenized_num_train\n",
    "shared_vocab, rev_shared_vocab = build_vocabulary(all_tokenized_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "max_length = 20  # found by introspection of our training set\n",
    "\n",
    "def vectorize_corpus(source_sequences, target_sequences, shared_vocab,\n",
    "                     word_level_source=True, word_level_target=True,\n",
    "                     max_length=max_length):\n",
    "    assert len(source_sequences) == len(target_sequences)\n",
    "    n_sequences = len(source_sequences)\n",
    "    source_ids = np.empty(shape=(n_sequences, max_length), dtype=np.int32)\n",
    "    source_ids.fill(shared_vocab[PAD])\n",
    "    target_ids = np.empty(shape=(n_sequences, max_length), dtype=np.int32)\n",
    "    target_ids.fill(shared_vocab[PAD])\n",
    "    numbered_pairs = zip(range(n_sequences), source_sequences, target_sequences)\n",
    "    for i, source_seq, target_seq in numbered_pairs:\n",
    "        source_tokens = tokenize(source_seq, word_level=word_level_source)\n",
    "        target_tokens = tokenize(target_seq, word_level=word_level_target)\n",
    "        \n",
    "        in_tokens, out_tokens = make_input_output(source_tokens, target_tokens)\n",
    "        \n",
    "        in_token_ids = [shared_vocab.get(t, UNK) for t in in_tokens]\n",
    "        source_ids[i, -len(in_token_ids):] = in_token_ids\n",
    "    \n",
    "        out_token_ids = [shared_vocab.get(t, UNK) for t in out_tokens]\n",
    "        target_ids[i, -len(out_token_ids):] = out_token_ids\n",
    "    return source_ids, target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = vectorize_corpus(fr_train, num_train, shared_vocab,\n",
    "                                    word_level_target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fr_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good. In particular we can note:\n",
    "\n",
    "- the PAD=0 symbol at the beginning of the two sequences,\n",
    "- the input sequence has the GO=1 symbol to separate the source from the target,\n",
    "- the output sequence is a shifted version of the target and ends with EOS=2.\n",
    "\n",
    "Let's vectorize the validation and test set to be able to evaluate our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_val, Y_val = vectorize_corpus(fr_val, num_val, shared_vocab,\n",
    "                                word_level_target=False)\n",
    "X_test, Y_test = vectorize_corpus(fr_test, num_test, shared_vocab,\n",
    "                                  word_level_target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_val.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple homogeneous Seq2Seq architecture\n",
    "\n",
    "\n",
    "To keep the architecture simple we will use the same RNN architecture and weights for the encoder part (before the `_GO` token) and the decoder part (after the `_GO` token).\n",
    "\n",
    "Here we use the GRU recurrent cell instead of LSTM because it is slightly faster to compute and should give comparable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dropout, GRU, Dense\n",
    "\n",
    "vocab_size = len(shared_vocab)\n",
    "simple_seq2seq = Sequential()\n",
    "simple_seq2seq.add(Embedding(vocab_size, 32, input_length=max_length))\n",
    "simple_seq2seq.add(Dropout(0.2))\n",
    "simple_seq2seq.add(GRU(128, return_sequences=True))\n",
    "simple_seq2seq.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "# Here we use the sparse_categorical_crossentropy loss to be able to pass\n",
    "# integer coded output for the token ids without having to \n",
    "simple_seq2seq.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simple_seq2seq.fit(X_train, Y_train[:, :, np.newaxis],\n",
    "                   validation_data=(X_val, Y_val[:, :, np.newaxis]),\n",
    "                   nb_epoch=1000, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at a raw prediction on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fr_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = simple_seq2seq.predict(X_test[0:1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def translate(model, source_sequence, shared_vocab, rev_shared_vocab,\n",
    "              word_level_source=True, word_level_target=True):\n",
    "    max_length = simple_seq2seq.input_shape[1]\n",
    "    source_tokens = tokenize(source_sequence, word_level=word_level_source)\n",
    "    input_ids = [shared_vocab.get(t, UNK) for t in source_tokens[::-1]]\n",
    "    input_ids += [shared_vocab[GO]]\n",
    "    decoded = []\n",
    "    while len(input_ids) <= max_length:\n",
    "        X = np.zeros(shape=(1, max_length), dtype=np.int32)\n",
    "        X[0, -len(input_ids):] = input_ids\n",
    "        next_id = model.predict(X)[0, -1].argmax()\n",
    "        if next_id == shared_vocab[EOS]:\n",
    "            break\n",
    "        decoded.append(rev_shared_vocab[next_id])\n",
    "        input_ids.append(next_id)\n",
    "    return \" \".join(decoded) if word_level_target else \"\".join(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "translate(simple_seq2seq, \"onze\", shared_vocab, rev_shared_vocab,\n",
    "          word_level_target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "translate(simple_seq2seq, \"cent trente deux\", shared_vocab, rev_shared_vocab,\n",
    "          word_level_target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "translate(simple_seq2seq, \"mille cent trente deux\", shared_vocab, rev_shared_vocab,\n",
    "          word_level_target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "translate(simple_seq2seq, \"quatre mille cent trente deux\", shared_vocab, rev_shared_vocab,\n",
    "          word_level_target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "translate(simple_seq2seq, \"quatre mille deux cent deux\", shared_vocab, rev_shared_vocab,\n",
    "          word_level_target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "translate(simple_seq2seq, \"vingt quatre\", shared_vocab, rev_shared_vocab,\n",
    "          word_level_target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "translate(simple_seq2seq, \"treize mille deux cent trois\", shared_vocab, rev_shared_vocab,\n",
    "          word_level_target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
