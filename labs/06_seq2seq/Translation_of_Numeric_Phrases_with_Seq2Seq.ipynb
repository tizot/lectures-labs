{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Translation of Numeric Phrases with Seq2Seq\n",
    "\n",
    "In the following we will try to build a **translation model from french phrases describing numbers** to the corresponding **numeric representation** (base 10).\n",
    "\n",
    "This is a toy machine translation with a **restricted vocabulary** and a **single valid translation for each source phrase** which makes it more tractable to train on a laptop computer and easier to evaluate. Despite those limitations we expect that this task highlight interested properties of Seq2Seq models including\n",
    "\n",
    "- the ability to deal with source and target sequences with different lengths,\n",
    "- token with a meaning that changes depending on the context (e.g \"quatre\" vs \"quatre vingts\" in \"quatre cents\"),\n",
    "- basic counting and \"reasoning\" capabilities of LSTM and GRU models.\n",
    "\n",
    "The parallel text data is generated from a \"ground-truth\" Python function named `to_french_phrase` that captures common rules from the French language except hypenation to make the French strings more ambiguous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from french_numbers import to_french_phrase\n",
    "\n",
    "\n",
    "for x in [21, 80, 81, 300, 213, 1100, 1201, 301000, 80080]:\n",
    "    print(str(x).rjust(6), to_french_phrase(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Generating a Training Set\n",
    "\n",
    "The following will **generate phrases 20000 example phrases for numbers between 1 and 1,000,000** (excluded). We chose to over-represent small numbers by generating all the possible short sequences between 1 and `exhaustive`.\n",
    "\n",
    "We then split the generated set into non-overlapping train, validation and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from french_numbers import generate_translations\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "numbers, french_numbers = generate_translations(\n",
    "    low=1, high=int(1e6) - 1, exhaustive=5000, random_seed=0)\n",
    "num_train, num_dev, fr_train, fr_dev = train_test_split(\n",
    "    numbers, french_numbers, test_size=0.5, random_state=0)\n",
    "\n",
    "num_val, num_test, fr_val, fr_test = train_test_split(\n",
    "    num_dev, fr_dev, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "len(fr_train), len(fr_val), len(fr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i, fr_phrase, num_phrase in zip(range(5), fr_train, num_train):\n",
    "    print(num_phrase.rjust(6), fr_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i, fr_phrase, num_phrase in zip(range(5), fr_val, num_val):\n",
    "    print(num_phrase.rjust(6), fr_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vocabularies\n",
    "\n",
    "Build the vocabularies from the training set only to get a chance to have some out-of-vocabulary words in the validation and test sets.\n",
    "\n",
    "First we need to introduce specific symbols that will be used to:\n",
    "- pad sequences\n",
    "- mark the beginning of translation\n",
    "- mark the end of translation\n",
    "- be used as a placehold for out-of-vocabulary symbols (not seen in the training set).\n",
    "\n",
    "Here we use the same convention as the [tensorflow seq2seq tutorial](https://www.tensorflow.org/tutorials/seq2seq):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "PAD, GO, EOS, UNK = START_VOCAB = ['_PAD', '_GO', '_EOS', '_UNK']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To build the vocabulary we need to tokenize the sequences of symbols. For the digital number representation we use character level tokenization while whitespace-based word level tokenization will do for the French phrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def tokenize(sentence, word_level=True):\n",
    "    if word_level:\n",
    "        return sentence.split()\n",
    "    else:\n",
    "        return [sentence[i:i + 1] for i in range(len(sentence))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tokenize('1234', word_level=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tokenize('mille deux cent trente quatre', word_level=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's now use this tokenization strategy to assign a unique integer token id to each possible token string found the traing set in each language ('French' and 'numeric'): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(tokenized_sequences):\n",
    "    rev_vocabulary = START_VOCAB[:]\n",
    "    unique_tokens = set()\n",
    "    for tokens in tokenized_sequences:\n",
    "        unique_tokens.update(tokens)\n",
    "    rev_vocabulary += sorted(unique_tokens)\n",
    "    vocabulary = {}\n",
    "    for i, token in enumerate(rev_vocabulary):\n",
    "        vocabulary[token] = i\n",
    "    return vocabulary, rev_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tokenized_fr_train = [tokenize(s, word_level=True) for s in fr_train]\n",
    "tokenized_num_train = [tokenize(s, word_level=False) for s in num_train]\n",
    "\n",
    "fr_vocab, rev_fr_vocab = build_vocabulary(tokenized_fr_train)\n",
    "num_vocab, rev_num_vocab = build_vocabulary(tokenized_num_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The two languages do not have the same vocabulary sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "len(fr_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "len(num_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for k, v in sorted(fr_vocab.items())[:10]:\n",
    "    print(k.rjust(10), v)\n",
    "print('...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for k, v in sorted(num_vocab.items()):\n",
    "    print(k.rjust(10), v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We also built the reverse mappings from token ids to token string representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(rev_fr_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(rev_num_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Seq2Seq with a single GRU architecture\n",
    "\n",
    "<img src=\"images/basic_seq2seq.png\" width=\"80%\" />\n",
    "\n",
    "From: [Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. \"Sequence to sequence learning with neural networks.\" NIPS 2014](https://arxiv.org/abs/1409.3215)\n",
    "\n",
    "\n",
    "\n",
    "For a given source sequence - target sequence pair, we will:\n",
    "- tokenize the source and target sequences;\n",
    "- reverse the order of the source sequence;\n",
    "- build the input sequence by concatenating the reversed source sequence and the target sequence in original order using the `_GO` token as a delimiter, \n",
    "- build the output sequence by appending the `_EOS` token to the source sequence.\n",
    "\n",
    "\n",
    "Let's do this as a function using the original string representations for the tokens so as to make it easier to debug:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def make_input_output(source_tokens, target_tokens, reverse_source=True):\n",
    "    if reverse_source:\n",
    "        source_tokens = source_tokens[::-1]\n",
    "    input_tokens = source_tokens + [GO] + target_tokens\n",
    "    output_tokens = target_tokens + [EOS]\n",
    "    return input_tokens, output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_tokens, output_tokens = make_input_output(\n",
    "    ['cent', 'vingt', 'et', 'un'],\n",
    "    ['1', '2', '1'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "output_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Vectorization of the parallel corpus\n",
    "\n",
    "Let's apply the previous transformation to each pair of (source, target) sequene and use a shared vocabulary to store the results in numpy arrays of integer token ids, with padding on the left so that all input / output sequences have the same length: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_tokenized_sequences = tokenized_fr_train + tokenized_num_train\n",
    "shared_vocab, rev_shared_vocab = build_vocabulary(all_tokenized_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "max_length = 20  # found by introspection of our training set\n",
    "\n",
    "def vectorize_corpus(source_sequences, target_sequences, shared_vocab,\n",
    "                     word_level_source=True, word_level_target=True,\n",
    "                     max_length=max_length):\n",
    "    assert len(source_sequences) == len(target_sequences)\n",
    "    n_sequences = len(source_sequences)\n",
    "    source_ids = np.empty(shape=(n_sequences, max_length), dtype=np.int32)\n",
    "    source_ids.fill(shared_vocab[PAD])\n",
    "    target_ids = np.empty(shape=(n_sequences, max_length), dtype=np.int32)\n",
    "    target_ids.fill(shared_vocab[PAD])\n",
    "    numbered_pairs = zip(range(n_sequences), source_sequences, target_sequences)\n",
    "    for i, source_seq, target_seq in numbered_pairs:\n",
    "        source_tokens = tokenize(source_seq, word_level=word_level_source)\n",
    "        target_tokens = tokenize(target_seq, word_level=word_level_target)\n",
    "        \n",
    "        in_tokens, out_tokens = make_input_output(source_tokens, target_tokens)\n",
    "        \n",
    "        in_token_ids = [shared_vocab.get(t, UNK) for t in in_tokens]\n",
    "        source_ids[i, -len(in_token_ids):] = in_token_ids\n",
    "    \n",
    "        out_token_ids = [shared_vocab.get(t, UNK) for t in out_tokens]\n",
    "        target_ids[i, -len(out_token_ids):] = out_token_ids\n",
    "    return source_ids, target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = vectorize_corpus(fr_train, num_train, shared_vocab,\n",
    "                                    word_level_target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fr_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This looks good. In particular we can note:\n",
    "\n",
    "- the PAD=0 symbol at the beginning of the two sequences,\n",
    "- the input sequence has the GO=1 symbol to separate the source from the target,\n",
    "- the output sequence is a shifted version of the target and ends with EOS=2.\n",
    "\n",
    "Let's vectorize the validation and test set to be able to evaluate our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_val, Y_val = vectorize_corpus(fr_val, num_val, shared_vocab,\n",
    "                                word_level_target=False)\n",
    "X_test, Y_test = vectorize_corpus(fr_test, num_test, shared_vocab,\n",
    "                                  word_level_target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_val.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### A simple homogeneous Seq2Seq architecture\n",
    "\n",
    "\n",
    "To keep the architecture simple we will use the same RNN architecture and weights for the encoder part (before the `_GO` token) and the decoder part (after the `_GO` token).\n",
    "\n",
    "Here we use the GRU recurrent cell instead of LSTM because it is slightly faster to compute and should give comparable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dropout, GRU, Dense\n",
    "\n",
    "vocab_size = len(shared_vocab)\n",
    "simple_seq2seq = Sequential()\n",
    "simple_seq2seq.add(Embedding(vocab_size, 32, input_length=max_length))\n",
    "simple_seq2seq.add(Dropout(0.2))\n",
    "simple_seq2seq.add(GRU(256, return_sequences=True))\n",
    "simple_seq2seq.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "# Here we use the sparse_categorical_crossentropy loss to be able to pass\n",
    "# integer-coded output for the token ids without having to convert to one-hot\n",
    "# codes\n",
    "simple_seq2seq.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's use a callback mechanism to automatically snapshot the best model found so far on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "best_model_fname = \"simple_seq2seq_checkpoint.h5\"\n",
    "best_model_cb = ModelCheckpoint(best_model_fname, monitor='val_loss',\n",
    "                                save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We need to use np.expand_dims trick on Y: this is required by Keras because of we use a sparse (integer-based) representation for the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = simple_seq2seq.fit(X_train, np.expand_dims(Y_train, -1),\n",
    "                             validation_data=(X_val, np.expand_dims(Y_val, -1)),\n",
    "                             nb_epoch=15, verbose=2, batch_size=32,\n",
    "                             callbacks=[best_model_cb])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], '--', label='validation')\n",
    "plt.ylabel('negative log likelihood')\n",
    "plt.xlabel('epoch')\n",
    "plt.title('Convergence plot for Simple Seq2Seq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's load the best model found on the validation at the end of training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "simple_seq2seq = load_model(best_model_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If you don't have access to a GPU and cannot wait 10 minutes to for the model to converge to a reasonably good state, feel to use the pretrained model. This model has been obtained by training the above model for ~150 epochs. The validation loss is significantly lower than 1e-5. In practice it should hardly ever make any prediction error on this easy translation problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# from keras.utils.data_utils import get_file\n",
    "# get_file(\"simple_seq2seq_pretrained.h5\", \n",
    "#          \"https://github.com/m2dsupsdlclass/lectures-labs/releases/\"\n",
    "#          \"download/0.4/simple_seq2seq_pretrained.h5\")\n",
    "# simple_seq2seq = load_model(\"simple_seq2seq_pretrained.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively you can load this imperfect model (trained only to 50 epochs) with a validation loss of ~7e-4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from keras.utils.data_utils import get_file\n",
    "# get_file(\"simple_seq2seq_partially_pretrained.h5\", \n",
    "#          \"https://github.com/m2dsupsdlclass/lectures-labs/releases/\"\n",
    "#          \"download/0.4/simple_seq2seq_partially_pretrained.h5\")\n",
    "# simple_seq2seq = load_model(\"simple_seq2seq_partially_pretrained.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's have a look at a raw prediction on the first sample of the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fr_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In numeric array this is provided (along with the expected target sequence) as the following padded input sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "first_test_sequence = X_test[0:1]\n",
    "first_test_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Remember that the `_GO` at `1` separates the reversed source from the expected target sequence:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rev_shared_vocab[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's feed this test sequence into the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "prediction = simple_seq2seq.predict(first_test_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's use `argmax` to extract the predicted token ids at each step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predicted_token_ids = prediction[0].argmax(-1)\n",
    "predicted_token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can use the shared reverse vocabulary to map this back to the string representation of the tokens: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "[rev_shared_vocab[token_id] for token_id in predicted_token_ids\n",
    " if token_id not in (shared_vocab[PAD], shared_vocab[EOS])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "However we cheate a bit because we gave the complete sequence and the solution in the input sequence. To correctly predict we need to predict one token at a time and reinject the predicted token in the input sequence to predict the next token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def greedy_translate(model, source_sequence, shared_vocab, rev_shared_vocab,\n",
    "                     word_level_source=True, word_level_target=True):\n",
    "    \"\"\"Greedy decoder recursively predicting one token at a time\"\"\"\n",
    "    # Initialize the list of input token ids with the source sequence\n",
    "    source_tokens = tokenize(source_sequence, word_level=word_level_source)\n",
    "    input_ids = [shared_vocab.get(t, UNK) for t in source_tokens[::-1]]\n",
    "    input_ids += [shared_vocab[GO]]\n",
    "\n",
    "    # Prepare a fixed size numpy array that matches the expected input\n",
    "    # shape for the model\n",
    "    input_array = np.empty(shape=(1, simple_seq2seq.input_shape[1]),\n",
    "                           dtype=np.int32)\n",
    "    decoded_tokens = []\n",
    "    while len(input_ids) <= max_length:\n",
    "        # Vectorize a the list of input tokens as \n",
    "        # and use zeros padding.\n",
    "        input_array.fill(shared_vocab[PAD])\n",
    "        input_array[0, -len(input_ids):] = input_ids\n",
    "        \n",
    "        # Predict the next output: greedy decoding with argmax\n",
    "        next_token_id = model.predict(input_array)[0, -1].argmax()\n",
    "        \n",
    "        # Stop decoding if the network predicts end of sentence:\n",
    "        if next_token_id == shared_vocab[EOS]:\n",
    "            break\n",
    "            \n",
    "        # Otherwise use the reverse vocabulary to map the prediction\n",
    "        # back to the string space\n",
    "        decoded_tokens.append(rev_shared_vocab[next_token_id])\n",
    "        \n",
    "        # Append prediction to input sequence to predict the next\n",
    "        input_ids.append(next_token_id)\n",
    "\n",
    "    separator = \" \" if word_level_target else \"\"\n",
    "    return separator.join(decoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "phrases = [\n",
    "    \"un\",\n",
    "    \"deux\",\n",
    "    \"trois\",\n",
    "    \"onze\",\n",
    "    \"quinze\",\n",
    "    \"cent trente deux\",\n",
    "    \"cent mille douze\",\n",
    "    \"vingt et un\",\n",
    "    \"vingt quatre\",\n",
    "    \"quatre vingts\",\n",
    "    \"quatre vingt onze mille\",\n",
    "    \"quatre vingt onze mille deux cent deux\",\n",
    "]\n",
    "for phrase in phrases:\n",
    "    translation = greedy_translate(simple_seq2seq, phrase,\n",
    "                                   shared_vocab, rev_shared_vocab,\n",
    "                                   word_level_target=False)\n",
    "    print(phrase.ljust(40), translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "phrases = [\n",
    "    \"quatre vingt et un\",\n",
    "    \"quarante douze\",\n",
    "    \"onze cent soixante vingt quatorze\",\n",
    "]\n",
    "for phrase in phrases:\n",
    "    translation = greedy_translate(simple_seq2seq, phrase,\n",
    "                                   shared_vocab, rev_shared_vocab,\n",
    "                                   word_level_target=False)\n",
    "    print(phrase.ljust(40), translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model evaluation\n",
    "\n",
    "Because **we expect only one correct translation** for a given source sequence, we can use **phrase-level accuracy** as a metric to quantify our model quality.\n",
    "\n",
    "Note that **this is not the case for real translation models** (e.g. from French to English on arbitrary sentences). Evaluation of a machine translation model is tricky in general. Automated evaluation can somehow be done at the corpus level with the [BLEU score](https://en.wikipedia.org/wiki/BLEU) (bilingual evaluation understudy) given a large enough sample of correct translations provided by certified translators but its only a noisy proxy.\n",
    "\n",
    "The only good evaluation is to give a large enough sample of the model predictions on some test sentences to certified translators and ask them to give an evaluation (e.g. a score between 0 and 6, 0 for non-sensical and 6 for the hypothetical perfect translation). However in practice this is very costly to do.\n",
    "\n",
    "Furtunately we can just use phrase-level accuracy on a our very domain specific toy problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def phrase_accuracy(model, num_sequences, fr_sequences, n_samples=300,\n",
    "                    decoder_func=greedy_translate):\n",
    "    correct = []\n",
    "    n_samples = len(num_sequences) if n_samples is None else n_samples\n",
    "    for i, num_seq, fr_seq in zip(range(n_samples), num_sequences, fr_sequences):\n",
    "        if i % 100 == 0:\n",
    "            print(\"Decoding %d/%d\" % (i, n_samples))\n",
    "\n",
    "        predicted_seq = decoder_func(simple_seq2seq, fr_seq,\n",
    "                                     shared_vocab, rev_shared_vocab,\n",
    "                                     word_level_target=False)\n",
    "        correct.append(num_seq == predicted_seq)\n",
    "    return np.mean(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Phrase-level test accuracy: %0.3f\"\n",
    "      % phrase_accuracy(simple_seq2seq, num_test, fr_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Phrase-level train accuracy: %0.3f\"\n",
    "      % phrase_accuracy(simple_seq2seq, num_train, fr_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding with a Beam Search\n",
    "\n",
    "Instead of decoding with greedy strategy that only considers the most-likely next token at each prediction, we can hold a priority queue of the most promising top-n sequences ordered by loglikelihoods.\n",
    "\n",
    "This could potentially improve the final accuracy of an imperfect model: indeed it can be the case that the most likely sequence (based on the conditional proability estimated by the model) starts with a character that is not the most likely alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def beam_translate(model, source_sequence, shared_vocab, rev_shared_vocab,\n",
    "                   word_level_source=True, word_level_target=True,\n",
    "                   beam_size=10, return_ll=False):\n",
    "    \"\"\"Decode candidate translations with a beam search strategy\n",
    "    \n",
    "    If return_ll is False, only the best candidate string is returned.\n",
    "    If return_ll is True, all the candidate strings and their loglikelihoods\n",
    "    are returned.\n",
    "    \"\"\"\n",
    "    # Initialize the list of input token ids with the source sequence\n",
    "    source_tokens = tokenize(source_sequence, word_level=word_level_source)\n",
    "    input_ids = [shared_vocab.get(t, UNK) for t in source_tokens[::-1]]\n",
    "    input_ids += [shared_vocab[GO]]\n",
    "    \n",
    "    # initialize loglikelihood, input token ids, decoded tokens for\n",
    "    # each candidate in the beam\n",
    "    candidates = [(0, input_ids[:], [], False)]\n",
    "\n",
    "    # Prepare a fixed size numpy array that matches the expected input\n",
    "    # shape for the model\n",
    "    input_array = np.empty(shape=(beam_size, simple_seq2seq.input_shape[1]),\n",
    "                           dtype=np.int32)\n",
    "    while any([not done and (len(input_ids) < max_length)\n",
    "               for _, input_ids, _, done in candidates]):\n",
    "        # Vectorize a the list of input tokens and use zeros padding.\n",
    "        input_array.fill(shared_vocab[PAD])\n",
    "        for i, (_, input_ids, _, done) in enumerate(candidates):\n",
    "            if not done:\n",
    "                input_array[i, -len(input_ids):] = input_ids\n",
    "        \n",
    "        # Predict the next output in a single call to the model to amortize\n",
    "        # the overhead and benefit from vector data parallelism on GPU.\n",
    "        next_likelihood_batch = model.predict(input_array)\n",
    "        \n",
    "        # Build the new candidates list by summing the loglikelood of the\n",
    "        # next token with their parents for each new possible expansion.\n",
    "        new_candidates = []\n",
    "        for i, (ll, input_ids, decoded, done) in enumerate(candidates):\n",
    "            if done:\n",
    "                new_candidates.append((ll, input_ids, decoded, done))\n",
    "            else:\n",
    "                next_loglikelihoods = np.log(next_likelihood_batch[i, -1])\n",
    "                for next_token_id, next_ll in enumerate(next_loglikelihoods):\n",
    "                    new_ll = ll + next_ll\n",
    "                    new_input_ids = input_ids[:]\n",
    "                    new_input_ids.append(next_token_id)\n",
    "                    new_decoded = decoded[:]\n",
    "                    new_done = done\n",
    "                    if next_token_id == shared_vocab[EOS]:\n",
    "                        new_done = True\n",
    "                    if not new_done:\n",
    "                        new_decoded.append(rev_shared_vocab[next_token_id])\n",
    "                    new_candidates.append(\n",
    "                        (new_ll, new_input_ids, new_decoded, new_done))\n",
    "        \n",
    "        # Only keep a beam of the most promising candidates\n",
    "        new_candidates.sort(reverse=True)\n",
    "        candidates = new_candidates[:beam_size]\n",
    "\n",
    "    separator = \" \" if word_level_target else \"\"\n",
    "    if return_ll:\n",
    "        return [(separator.join(decoded), ll) for ll, _, decoded, _ in candidates]\n",
    "    else:\n",
    "        _, _, decoded, done = candidates[0]\n",
    "        return separator.join(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "candidates = beam_translate(simple_seq2seq, \"cent mille un\",\n",
    "                            shared_vocab, rev_shared_vocab,\n",
    "                            word_level_target=False,\n",
    "                            return_ll=True, beam_size=10)\n",
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "candidates = beam_translate(simple_seq2seq, \"quatre vingts\",\n",
    "                            shared_vocab, rev_shared_vocab,\n",
    "                            word_level_target=False,\n",
    "                            return_ll=True, beam_size=10)\n",
    "candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Accuracy with Beam Search Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Phrase-level test accuracy: %0.3f\"\n",
    "      % phrase_accuracy(simple_seq2seq, num_test, fr_test,\n",
    "                        decoder_func=beam_translate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Phrase-level train accuracy: %0.3f\"\n",
    "      % phrase_accuracy(simple_seq2seq, num_train, fr_train,\n",
    "                        decoder_func=beam_translate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the partially trained model the test phrase-level is slightly better (0.38 vs 0.37) with the beam decoder than with greedy decoder but the change is not that important on our toy task. Training the model to covergence would yield perfect scores anyway.\n",
    "\n",
    "Properly tuned beam search decoding can be critical to improve the quality of Machine Translation systems trained on natural language pairs though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
